{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "This coding environment will be used to train the agent for the project.\n",
    "\n",
    "## 1. Intro\n",
    "\n",
    "### Start the environment\n",
    "\n",
    "The next code cell installs a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n",
      "CPU times: user 826 ms, sys: 119 ms, total: 945 ms\n",
      "Wall time: 50.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_env(unity_file):\n",
    "    # Initialize the environment\n",
    "    env = UnityEnvironment(file_name=unity_file)\n",
    "\n",
    "    # Get default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    # Get state and action spaces\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state_size = env_info.vector_observations.shape[1]\n",
    "    action_size = brain.vector_action_space_size\n",
    "    n_agents = len(env_info.agents)\n",
    "    \n",
    "    print('State size: ', state_size)\n",
    "    print('Action size: ', action_size)\n",
    "    print('Number of agents: ', n_agents)\n",
    "    \n",
    "    return env, brain_name, brain, state_size, action_size, n_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size:  33\n",
      "Action size:  4\n",
      "Number of agents:  20\n"
     ]
    }
   ],
   "source": [
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# unity_file = '/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64'\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "unity_file='/data/Reacher_Linux_NoVis/Reacher.x86_64'\n",
    "\n",
    "env, brain_name, brain, state_size, action_size, n_agents = initialize_env(unity_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the State and Action Spaces\n",
    "\n",
    "Let's print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   7.90150833e+00  -1.00000000e+00\n",
      "   1.25147629e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -5.22214413e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'\n",
    "      .format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Random Actions in the Environment\n",
    "\n",
    "**In this coding environment, we will not be able to watch the agents while they are training**, and we should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0859999980777502. Number of episodes: 1000.\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "i_episode = 0\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    i_episode += 1\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}. Number of episodes: {}.'\n",
    "      .format(np.mean(scores), i_episode-1))\n",
    "\n",
    "# Just to be sure they're not used any more\n",
    "del states, actions, next_states, rewards, dones, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "#### Policy-Based Methods\n",
    "\n",
    "Value-based methods can only deal with comparably small state spaces.\n",
    "A table with values of states and actions is filled through the agent's interaction with the environment.\n",
    "Thereby the optimal value function $q_*$ helps to find the optimal policy $\\pi_*$.\n",
    "For bigger (also: continuous) state spaces deep neural networks are used to learn the optimal action-value network, which is known as the Deep Q-Learning algorithm.\n",
    "\n",
    "Policy-based methods, in contrast to that, try to estimate the optimal policy without worrying about the optimal value function.\n",
    "Methods such as Hill Climbing, Gradient Ascend or Stochastic Policy Search can be used for Policy Function Approximation. The so called Cross-Entropy Method and Evolution Strategies are further algorithms in this field.\n",
    "\n",
    "Policy-Based methods are helpful, since they 1.) are simpler than value-based methods, 2.) are able to learn true stochastic policies and 3.) are able to deal with continuous action spaces.\n",
    "\n",
    "#### Actor-Critic Methods\n",
    "\n",
    "The general idea of actor-critic methods is to use value-based methods to further reduce the variance of policy-based methods.\n",
    "One uses two function approximators (typically neural networks) to learn a policy (the Actor) and a value function (the Critic); the actor is learning to act, whereas the critic is learning to estimate situations and actions.\n",
    "This combination typically results in more stable learning than value-based agents alone, but also fewer samples are necessary than for policy-based agents alone.\n",
    "\n",
    "A basic Actor-Critic agent works as follows:\n",
    " 1. Actor: Observe state $s$ of the environment\n",
    " 1. Actor: Calculate distribution of action probabilities $\\pi(a \\mid s; \\theta_{\\pi})$, select action and execute\n",
    " 1. Actor: Observe experience tuple of state, action, reward and next state: $o=(s, a, r, s')$\n",
    " 1. Critic: Use the tuple $o$ for TD-estimate, to train critic: $r + \\gamma V(s'; \\theta_{V})$\n",
    " 1. Critic: Calculate the advantage $A(s, a) = r + \\gamma V(s'; \\theta_{V}) - V(s; \\theta_{V}) $\n",
    " 1. Actor: Train the actor using the advantage\n",
    "\n",
    "Common algorithms are\n",
    " - A3C (Asynchronous Advantage Actor-Critic)\n",
    " - A2C (Advantage Actor-Critic)\n",
    " - Generalized Advantage Estimation\n",
    " - DDPG (Deep Deterministic Policy Gradient)\n",
    "\n",
    "#### Deep Deterministic Policy Gradient\n",
    "\n",
    "DDPG has been introduced in Lillicrap et al., 2016.\n",
    "It is being used in the implementation in the following.\n",
    "\n",
    "DDPG combines the actor-critic approach with Deep Q-Learning.\n",
    "It can be seen as a kind of approximate DQN instead of an actual actor-critic.\n",
    "The actor provides the current policy, mapping states to continuous deterministic actions.\n",
    "The critic is used to calculate action values.\n",
    "DDPG is using a replay buffer (like DQN), and target networks.\n",
    "This means, there are 2 copies of the network weights for each network (regular+target for actor, regular+target for critic), where the targets are updated using a soft update strategy.\n",
    "The target network is used for prediction to stabilize training, whereas the regular network is the one that is being trained.\n",
    "\n",
    "For scaling batch normalization is being applied in the models.\n",
    "This normalizes each dimension across the samples of the minibatch.\n",
    "\n",
    "Process Noise is added in order to handle exploration (Ornstein-Uhlenbeck noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "> The following solution is based on the code provided in [Udacity ddpg-bipedal](https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-bipedal/DDPG.ipynb). In particular, it uses the files [model.py](https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-bipedal/model.py) and [ddpg_agent/py](https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-bipedal/ddpg_agent.py), as well as the code provided in the training loop `ddpg()`.\n",
    "\n",
    "####  Neural Network Architecture\n",
    "\n",
    "TODO - Finalize...\n",
    "After trying the initial architecture from the Udacity bipolar walker, and reading in the student hub I adapted the model as follows:\n",
    "\n",
    "For the **actor** I added another fully connected layer with 128 neurons.\n",
    "This results in the following number of neurons on each layer then (input, FC1, FC2, output): `33-128-128-4`.\n",
    "Besides, a batch normalization layer is being introduced in between the fully connected layers, to help normalize the activation across the batch of samples from the replay buffer\n",
    "\n",
    "The **critic** is adjusted in the following way:\n",
    "128-128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDPG Algorithm Implementation\n",
    "\n",
    "The initial [hyperparameter search](####Hyperparameter-grid-search) has been conducted with a slightly different implementation, adjusted for the 1-agent environment (for details, see ([previous state ongithub](https://github.com/toebgen/deep-reinforcement-learning/commit/3444b51e7e4b07696ad717595aaa60968631d18a)).\n",
    "The current implementation works with the 20-agent environenment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime, time\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from workspace_utils import active_session\n",
    "\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(env, brain_name, agent, n_agents,\n",
    "         n_episodes=750, t_max=700):\n",
    "    \n",
    "    scores = []\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    last_average_score = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        t_start = datetime.datetime.now()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations\n",
    "        agent.reset()\n",
    "        score = np.zeros(n_agents)\n",
    "        \n",
    "        for t in range(t_max):\n",
    "            print('\\rEpisode {}/{}, t: {}/{}'\n",
    "                  .format(i_episode, n_episodes, t, t_max), end=\"\")\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            env_info = env.step(action)[brain_name]    # send all actions to the environment\n",
    "            next_state = env_info.vector_observations  # get next state (for each agent)\n",
    "            reward = env_info.rewards                  # get reward (for each agent)\n",
    "            done = env_info.local_done                 # see if episode finished\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done, t)\n",
    "            \n",
    "            score += reward\n",
    "            state = next_state      # roll over states to next time step\n",
    "            \n",
    "            if np.any(done):        # exit loop if episode finished\n",
    "                break\n",
    "                \n",
    "        average_score = np.mean(score)\n",
    "        scores_deque.append(average_score)\n",
    "        scores.append(average_score)\n",
    "        \n",
    "        t_episode = datetime.datetime.now() - t_start\n",
    "        print('\\rEpisode: {}, Average Score: {:.2f}, Score: {:.2f}, time: {}'\n",
    "              .format(i_episode, np.mean(scores_deque),\n",
    "              average_score, t_episode), end=\"\")\n",
    "\n",
    "        # average_score should be above 5. after 100 episodes! Abort if not...\n",
    "        # if i_episode >= 100:\n",
    "        #     if average_score < 5.:\n",
    "        #         print('\\nAverage score is only {:.3f} after {} episodes, '\\\n",
    "        #             'hence aborting this run!\\n'.format(average_score, i_episode))\n",
    "        #         break\n",
    "        \n",
    "        if i_episode % 10 == 0:\n",
    "            print('')\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "\n",
    "        if np.mean(scores_deque) >= 30.0:\n",
    "            print('\\n\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'\n",
    "                  .format(i_episode-100, np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor_solution.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic_solution.pth')\n",
    "            break\n",
    "            \n",
    "    return scores, np.mean(scores_deque), t_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several hyperparameters for training the DDPG algorithm.\n",
    "Some of them are listed in the following cell, including possible values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 400  # Should be enough to solve this!\n",
    "BUFFER_SIZE = [int(1e5), int(1e7)]\n",
    "BATCH_SIZE = [64, 128, 256]\n",
    "GAMMA = .99\n",
    "TAU = 1e-3\n",
    "LEARN_RATE = [1e-3, 1e-4]\n",
    "WEIGHT_DECAY = 0.0\n",
    "SEED = 2\n",
    "T_MAX = [1000, 3000]\n",
    "NEURONS = [128, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter grid search\n",
    "\n",
    "Manual try&error of a few hyperparameters did unfortunately not lead to a succesful model.\n",
    "Hence, in order to find the best hyperparameters, we define some possible values and perform a grid search over the hyperparameter space.\n",
    "It shall be pointed out, that the `ddpg()` method exits, if the score did not reach at least `5.0` after `100` episodes!\n",
    "\n",
    "First, let's create all combinations of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be testing 48 combinations of hyperparameters!\n",
      "3 random Examples:\n",
      "Combination(buffer_size=100000, batch_size=128, learn_rate=0.0001, t_max=1000, neurons=128)\n",
      "Combination(buffer_size=10000000, batch_size=256, learn_rate=0.0001, t_max=3000, neurons=128)\n",
      "Combination(buffer_size=10000000, batch_size=64, learn_rate=0.0001, t_max=1000, neurons=256)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from itertools import product, starmap\n",
    "import pickle\n",
    "\n",
    "Combination = namedtuple('Combination', 'buffer_size batch_size learn_rate t_max neurons')\n",
    "\n",
    "def named_product(**items):\n",
    "    # Combination = namedtuple('Combination', items.keys())\n",
    "    return starmap(Combination, product(*items.values()))\n",
    "\n",
    "hyperparameter_combinations = []\n",
    "for combination in named_product(buffer_size=BUFFER_SIZE,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 learn_rate=LEARN_RATE,\n",
    "                                 t_max=T_MAX,\n",
    "                                 neurons=NEURONS):\n",
    "    # print(combination)\n",
    "    hyperparameter_combinations.append(combination)\n",
    "\n",
    "len_hyperparameter_combinations = len(hyperparameter_combinations)\n",
    "print('We will be testing {} combinations of hyperparameters!'\n",
    "      .format(len_hyperparameter_combinations))\n",
    "\n",
    "# Let's check a few of them\n",
    "num_examples = 3\n",
    "print('{} random Examples:'.format(num_examples))\n",
    "for _ in range(num_examples):\n",
    "    print(hyperparameter_combinations[random.randint(0, len_hyperparameter_combinations-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grid_search(combinations_to_run):\n",
    "    with active_session():\n",
    "        agent = None\n",
    "        scores_grid_search = []\n",
    "\n",
    "        for i, hyperparameters in enumerate(hyperparameter_combinations):\n",
    "            id = i+1\n",
    "            if id not in combinations_to_run:\n",
    "                continue\n",
    "\n",
    "            print('Testing following hyperparameters ({}/{}):\\n{}\\n'\n",
    "                  .format(id, len(hyperparameter_combinations), hyperparameters))\n",
    "\n",
    "            # Delete any existing agent\n",
    "            if agent is not None:\n",
    "                del agent\n",
    "\n",
    "            # Initialize agent\n",
    "            agent = Agent(state_size,\n",
    "                          action_size,\n",
    "                          n_agents,\n",
    "                          buffer_size=hyperparameters.buffer_size,\n",
    "                          batch_size=hyperparameters.batch_size,\n",
    "                          gamma=GAMMA,\n",
    "                          tau=TAU,\n",
    "                          lr_actor=hyperparameters.learn_rate,\n",
    "                          lr_critic=hyperparameters.learn_rate,\n",
    "                          weight_decay=WEIGHT_DECAY,\n",
    "                          neurons=hyperparameters.neurons,\n",
    "                          random_seed=SEED)\n",
    "\n",
    "            # Run training\n",
    "            scores, average_score, t_episode = ddpg(env, brain_name, agent, n_agents,\n",
    "                                                    n_episodes=N, t_max=hyperparameters.t_max)\n",
    "\n",
    "            # Save results\n",
    "            print('Saving results...')\n",
    "            scores_grid_search.append((hyperparameters, t_episode, scores, average_score))\n",
    "            # TODO Write directly to CSV file...\n",
    "            \n",
    "        return scores_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_grid_search = False\n",
    "\n",
    "if run_grid_search:\n",
    "    combinations_to_run = [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 38]  #[44]  # Pick this as \"default\" parameters\n",
    "    scores_grid_search = grid_search(combinations_to_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment with 1 Agent\n",
    "\n",
    "Version `3444b51` ([github](https://github.com/toebgen/deep-reinforcement-learning/commit/3444b51e7e4b07696ad717595aaa60968631d18a)) reflects the used code for this hyperparameter search.\n",
    "It used the environment with only 1 agent.\n",
    "\n",
    "The output of the cell above has been manually put into the file `screen_scrape.txt`, and then processed by `create_csv.py`.\n",
    "Resulting CSV file will be imported in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>buffer_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>max_t</th>\n",
       "      <th>neurons</th>\n",
       "      <th>episodes</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>27</td>\n",
       "      <td>10000000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>1.37</td>\n",
       "      <td>00:00:14.567689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41</td>\n",
       "      <td>10000000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>1.29</td>\n",
       "      <td>00:00:21.143977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>37</td>\n",
       "      <td>10000000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>1.28</td>\n",
       "      <td>00:00:17.895239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>100000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>1.27</td>\n",
       "      <td>00:00:20.971953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>12</td>\n",
       "      <td>100000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>1.20</td>\n",
       "      <td>00:00:16.543660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>10000000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>1.19</td>\n",
       "      <td>00:00:17.244626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>28</td>\n",
       "      <td>10000000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>1.18</td>\n",
       "      <td>00:00:14.360792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>26</td>\n",
       "      <td>10000000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>1.18</td>\n",
       "      <td>00:00:15.332227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>44</td>\n",
       "      <td>10000000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>1.16</td>\n",
       "      <td>00:00:19.806695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>42</td>\n",
       "      <td>10000000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>1.11</td>\n",
       "      <td>00:00:19.747336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>34</td>\n",
       "      <td>10000000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>1.08</td>\n",
       "      <td>00:00:17.614199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>36</td>\n",
       "      <td>10000000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>1.05</td>\n",
       "      <td>00:00:17.472221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14</td>\n",
       "      <td>100000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>1.04</td>\n",
       "      <td>00:00:16.458676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>25</td>\n",
       "      <td>10000000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.99</td>\n",
       "      <td>00:00:14.644425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>43</td>\n",
       "      <td>10000000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.96</td>\n",
       "      <td>00:00:19.380926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30</td>\n",
       "      <td>10000000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>00:00:14.338409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>100000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.91</td>\n",
       "      <td>00:00:17.601879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>22</td>\n",
       "      <td>100000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.88</td>\n",
       "      <td>00:00:20.228565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>100000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.84</td>\n",
       "      <td>00:00:20.787934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17</td>\n",
       "      <td>100000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.83</td>\n",
       "      <td>00:00:20.479269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29</td>\n",
       "      <td>10000000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.78</td>\n",
       "      <td>00:00:15.984810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>100000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.76</td>\n",
       "      <td>00:00:17.392905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19</td>\n",
       "      <td>100000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>00:00:19.799508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.74</td>\n",
       "      <td>00:00:14.998592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>100000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.72</td>\n",
       "      <td>00:00:15.916030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>10000000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.72</td>\n",
       "      <td>00:00:18.075663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>100000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.70</td>\n",
       "      <td>00:00:15.915925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>45</td>\n",
       "      <td>10000000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.68</td>\n",
       "      <td>00:00:19.260393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>38</td>\n",
       "      <td>10000000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.67</td>\n",
       "      <td>00:00:16.133522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>46</td>\n",
       "      <td>10000000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.65</td>\n",
       "      <td>00:00:19.766807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>100000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.63</td>\n",
       "      <td>00:00:14.555644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.62</td>\n",
       "      <td>00:00:15.527081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>35</td>\n",
       "      <td>10000000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.57</td>\n",
       "      <td>00:00:17.164317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>10000000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.56</td>\n",
       "      <td>00:00:15.346309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.46</td>\n",
       "      <td>00:00:14.470509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>100000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.45</td>\n",
       "      <td>00:00:15.713651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>48</td>\n",
       "      <td>10000000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.41</td>\n",
       "      <td>00:00:20.135187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>24</td>\n",
       "      <td>100000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.39</td>\n",
       "      <td>00:00:20.332723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>100000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>00:00:16.483312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>100000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.29</td>\n",
       "      <td>00:00:15.633495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>23</td>\n",
       "      <td>100000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.20</td>\n",
       "      <td>00:00:20.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>10000000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.19</td>\n",
       "      <td>00:00:17.384141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>10000000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.16</td>\n",
       "      <td>00:00:15.489140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>47</td>\n",
       "      <td>10000000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.13</td>\n",
       "      <td>00:00:19.581966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21</td>\n",
       "      <td>100000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>00:00:20.144430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15</td>\n",
       "      <td>100000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.09</td>\n",
       "      <td>00:00:16.103063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>100000</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.07</td>\n",
       "      <td>00:00:16.152890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>100000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3000</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>00:00:17.303011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  buffer_size  batch_size  learn_rate  max_t  neurons  episodes  \\\n",
       "45  27     10000000          64      0.0010   3000      128       100   \n",
       "29  41     10000000         256      0.0010   1000      128       100   \n",
       "26  37     10000000         128      0.0001   1000      128       100   \n",
       "16  20       100000         256      0.0010   3000      256       100   \n",
       "37  12       100000         128      0.0010   3000      256       100   \n",
       "22  33     10000000         128      0.0010   1000      128       100   \n",
       "46  28     10000000          64      0.0010   3000      256       100   \n",
       "44  26     10000000          64      0.0010   1000      256       100   \n",
       "32  44     10000000         256      0.0010   3000      256       100   \n",
       "30  42     10000000         256      0.0010   1000      256       100   \n",
       "23  34     10000000         128      0.0010   1000      256       100   \n",
       "25  36     10000000         128      0.0010   3000      256       100   \n",
       "38  14       100000         128      0.0001   1000      256       100   \n",
       "43  25     10000000          64      0.0010   1000      128       100   \n",
       "31  43     10000000         256      0.0010   3000      128       100   \n",
       "19  30     10000000          64      0.0001   1000      256       100   \n",
       "9   10       100000         128      0.0010   1000      256        50   \n",
       "40  22       100000         256      0.0001   1000      256       100   \n",
       "14  18       100000         256      0.0010   1000      256        50   \n",
       "13  17       100000         256      0.0010   1000      128        50   \n",
       "18  29     10000000          64      0.0001   1000      128       100   \n",
       "8    9       100000         128      0.0010   1000      128        50   \n",
       "15  19       100000         256      0.0010   3000      128        50   \n",
       "2    3       100000          64      0.0010   3000      128        50   \n",
       "10  11       100000         128      0.0010   3000      128        50   \n",
       "28  40     10000000         128      0.0001   3000      256       100   \n",
       "5    6       100000          64      0.0001   1000      256        50   \n",
       "33  45     10000000         256      0.0001   1000      128       100   \n",
       "47  38     10000000         128      0.0001   1000      256       100   \n",
       "34  46     10000000         256      0.0001   1000      256       100   \n",
       "1    2       100000          64      0.0010   1000      256        50   \n",
       "3    4       100000          64      0.0010   3000      256        50   \n",
       "24  35     10000000         128      0.0010   3000      128       100   \n",
       "21  32     10000000          64      0.0001   3000      256       100   \n",
       "0    1       100000          64      0.0010   1000      128        50   \n",
       "7    8       100000          64      0.0001   3000      256        50   \n",
       "36  48     10000000         256      0.0001   3000      256       100   \n",
       "42  24       100000         256      0.0001   3000      256       100   \n",
       "11  13       100000         128      0.0001   1000      128        50   \n",
       "4    5       100000          64      0.0001   1000      128        50   \n",
       "41  23       100000         256      0.0001   3000      128       100   \n",
       "27  39     10000000         128      0.0001   3000      128       100   \n",
       "20  31     10000000          64      0.0001   3000      128       100   \n",
       "35  47     10000000         256      0.0001   3000      128       100   \n",
       "17  21       100000         256      0.0001   1000      128        50   \n",
       "39  15       100000         128      0.0001   3000      128       100   \n",
       "6    7       100000          64      0.0001   3000      128        50   \n",
       "12  16       100000         128      0.0001   3000      256        50   \n",
       "\n",
       "    avg_score             time  \n",
       "45       1.37  00:00:14.567689  \n",
       "29       1.29  00:00:21.143977  \n",
       "26       1.28  00:00:17.895239  \n",
       "16       1.27  00:00:20.971953  \n",
       "37       1.20  00:00:16.543660  \n",
       "22       1.19  00:00:17.244626  \n",
       "46       1.18  00:00:14.360792  \n",
       "44       1.18  00:00:15.332227  \n",
       "32       1.16  00:00:19.806695  \n",
       "30       1.11  00:00:19.747336  \n",
       "23       1.08  00:00:17.614199  \n",
       "25       1.05  00:00:17.472221  \n",
       "38       1.04  00:00:16.458676  \n",
       "43       0.99  00:00:14.644425  \n",
       "31       0.96  00:00:19.380926  \n",
       "19       0.92  00:00:14.338409  \n",
       "9        0.91  00:00:17.601879  \n",
       "40       0.88  00:00:20.228565  \n",
       "14       0.84  00:00:20.787934  \n",
       "13       0.83  00:00:20.479269  \n",
       "18       0.78  00:00:15.984810  \n",
       "8        0.76  00:00:17.392905  \n",
       "15       0.75  00:00:19.799508  \n",
       "2        0.74  00:00:14.998592  \n",
       "10       0.72  00:00:15.916030  \n",
       "28       0.72  00:00:18.075663  \n",
       "5        0.70  00:00:15.915925  \n",
       "33       0.68  00:00:19.260393  \n",
       "47       0.67  00:00:16.133522  \n",
       "34       0.65  00:00:19.766807  \n",
       "1        0.63  00:00:14.555644  \n",
       "3        0.62  00:00:15.527081  \n",
       "24       0.57  00:00:17.164317  \n",
       "21       0.56  00:00:15.346309  \n",
       "0        0.46  00:00:14.470509  \n",
       "7        0.45  00:00:15.713651  \n",
       "36       0.41  00:00:20.135187  \n",
       "42       0.39  00:00:20.332723  \n",
       "11       0.38  00:00:16.483312  \n",
       "4        0.29  00:00:15.633495  \n",
       "41       0.20  00:00:20.001375  \n",
       "27       0.19  00:00:17.384141  \n",
       "20       0.16  00:00:15.489140  \n",
       "35       0.13  00:00:19.581966  \n",
       "17       0.10  00:00:20.144430  \n",
       "39       0.09  00:00:16.103063  \n",
       "6        0.07  00:00:16.152890  \n",
       "12       0.05  00:00:17.303011  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "grid_search = pd.read_csv(\"grid_search.csv\")\n",
    "grid_search['time'] = pd.to_datetime(grid_search['time']).dt.time\n",
    "grid_search.sort_values(by='avg_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the table above, none of the hyperparameter combinations results in a score of more than `1.37` after the first `100` episodes.\n",
    "Several of the training runs have already been aborted after `50` episodes, since the score did not reach `1.0` by then.\n",
    "\n",
    "It is expected, that training reaches at least a score of `5.0` after `100` episodes, as can be seen in other students' submissions and as discussed on the Udacity Nanodegree Slack channnel.\n",
    "This holds even for the single agent environment.\n",
    "Hence, an error in the current implementation of the training algorithm is suspected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switching to 20-agent environment\n",
    "\n",
    "After reading up even more on the different agent environments it seems like the 20-agent environment is likely to converge quicker, hence the code has been slightly adapted to be able to work with 20 instead of only 1 agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination(buffer_size=10000000, batch_size=256, learn_rate=0.001, t_max=3000, neurons=128)\n",
    "# Episode: 10, Average Score: 0.32, Score: 0.56, time: 0:00:21.672011\n",
    "# Episode: 20, Average Score: 0.37, Score: 0.19, time: 0:00:25.505669\n",
    "# Episode: 30, Average Score: 0.50, Score: 1.65, time: 0:00:30.469980\n",
    "# Episode: 40, Average Score: 0.81, Score: 1.92, time: 0:00:35.663340\n",
    "# Episode: 50, Average Score: 1.22, Score: 2.96, time: 0:00:40.501980\n",
    "# Episode: 60, Average Score: 1.56, Score: 3.73, time: 0:00:45.807096\n",
    "# Episode: 70, Average Score: 1.92, Score: 3.72, time: 0:00:50.708950\n",
    "# Episode: 80, Average Score: 2.17, Score: 3.90, time: 0:00:55.982343\n",
    "\n",
    "# Combination(buffer_size=10000000, batch_size=256, learn_rate=0.001, t_max=3000, neurons=256)\n",
    "# Episode: 20, Average Score: 1.09, Score: 2.24, time: 0:00:26.294112\n",
    "# Episode: 40, Average Score: 2.60, Score: 5.43, time: 0:00:36.166855\n",
    "# Episode: 60, Average Score: 4.46, Score: 10.27, time: 0:00:46.378628\n",
    "# Episode: 80, Average Score: 6.26, Score: 13.00, time: 0:00:56.321592\n",
    "# Episode: 100, Average Score: 8.09, Score: 17.96, time: 0:01:05.983418\n",
    "# Episode: 120, Average Score: 11.41, Score: 14.97, time: 0:01:16.048861\n",
    "# Episode: 140, Average Score: 13.70, Score: 13.54, time: 0:01:25.720784\n",
    "# Episode: 180, Average Score: 15.34, Score: 14.11, time: 0:02:33.815713\n",
    "# Episode: 200, Average Score: 14.99, Score: 14.48, time: 0:02:30.486623\n",
    "# Episode: 220, Average Score: 14.12, Score: 13.30, time: 0:02:46.839219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2/2000, t: 282/30000.00, Score: 0.00, time: 0:00:30.805487"
     ]
    }
   ],
   "source": [
    "# Initialize agent\n",
    "agent = Agent(state_size,\n",
    "              action_size,\n",
    "              n_agents,\n",
    "              buffer_size=int(1e7),\n",
    "              batch_size=256,\n",
    "              gamma=.99,\n",
    "              tau=1e-3,\n",
    "              lr_actor=1e-4,\n",
    "              lr_critic=1e-3,\n",
    "              weight_decay=0.0,\n",
    "              neurons=128,\n",
    "              random_seed=2)\n",
    "\n",
    "# Run training\n",
    "scores, average_score, t_episode = ddpg(env, brain_name, agent, n_agents,\n",
    "                                        n_episodes=2000, t_max=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, we can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "The tuning of hyperparameters can be further improved.\n",
    "Possibly a framework like [hyperopt](https://github.com/hyperopt/hyperopt) or [optuna](https://optuna.org) can be used.\n",
    "\n",
    "Other algorithms for solving the problem might also be interesting, such as:\n",
    "- PPO\n",
    "- A3C\n",
    "- D4PG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
